{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Neural Network from Scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll create a NeuralNetwork class in Python to train the neuron to give an accurate prediction. The class will also have other helper functions.\n",
    "\n",
    "Even though we’ll not use a neural network library for this simple neural network example, we’ll import the numpy library to assist with the calculations.\n",
    "\n",
    "The library comes with the following four important methods:\n",
    "\n",
    "1. exp—for generating the natural exponential\n",
    "2. array—for generating a matrix\n",
    "3. dot—for multiplying matrices\n",
    "4. random—for generating random numbers. Note that we’ll seed the random numbers to ensure their efficient distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self):\n",
    "        # seeding for random number generation\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        #converting weights to a 3 by 1 matrix with values from -1 to 1 and mean of 0\n",
    "        self.synaptic_weights = 2 * np.random.random((3, 1)) - 1\n",
    "        \n",
    "        #Code out the sigmoid function, and the derivative of the sigmoid function below\n",
    "        \n",
    "        def sigmoid(self,x):\n",
    "            pass\n",
    "        \n",
    "        def sigmoid_derivative(self,x):\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "This is the stage where we’ll teach the neural network to make an accurate prediction. Every input will have a weight—either positive or negative.\n",
    "\n",
    "This implies that an input having a big number of positive weight or a big number of negative weight will influence the resulting output more.\n",
    "\n",
    "Remember that we initially began by allocating every weight to a random number.\n",
    "\n",
    "Here is the procedure for the training process we used in this neural network example problem:\n",
    "\n",
    "We take the inputs from the training dataset, perform some adjustments based on their weights, and siphon them via a method that computed the output of the ANN.\n",
    "\n",
    "We computed the back-propagated error rate. \n",
    "\n",
    "In this case, it is the difference between neuron’s predicted output and the expected output of the training dataset.\n",
    "\n",
    "Based on the extent of the error got, we performed some minor weight adjustments using the Error Weighted Derivative formula.\n",
    "\n",
    "We have to iterate this process an arbitrary number (very large number) of times. In every iteration, the whole training set is processed simultaneously.\n",
    "We used the “.T” function for transposing the matrix from horizontal position to vertical position. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def train(self, training_inputs, training_outputs, training_iterations):\n",
    "        \n",
    "        #training the model to make accurate predictions while adjusting weights continually\n",
    "        for iteration in range(training_iterations):\n",
    "            #siphon the training data via  the neuron\n",
    "            output = self.think(training_inputs)\n",
    "\n",
    "            #computing error rate for back-propagation\n",
    "            error = #Code error here\n",
    "            \n",
    "            #performing weight adjustments\n",
    "            adjustments = np.dot(training_inputs.T, error * self.sigmoid_derivative(output))\n",
    "\n",
    "            self.synaptic_weights += adjustments\n",
    "\n",
    "    def think(self, inputs):\n",
    "        #passing the inputs via the neuron to get output   \n",
    "        #converting values to floats\n",
    "        \n",
    "        pass\n",
    "        #Type code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "Finally, we initialize the NeuralNetwork class and run the code.\n",
    "\n",
    "We managed to create a simple neural network.\n",
    "\n",
    "The neuron began by allocating itself some random weights. \n",
    "Thereafter, it trained itself using the training examples.\n",
    "\n",
    "Consequently, if it was presented with a new situation [1,0,0], it gave the value of 0.9999584"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Randomly Generated Weights: \n",
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "Ending Weights After Training: \n",
      "[[10.08740896]\n",
      " [-0.20695366]\n",
      " [-4.83757835]]\n",
      "User Input One: 1\n",
      "User Input Two: 1.5\n",
      "User Input Three: 0.9\n",
      "Considering New Situation:  1 1.5 0.9\n",
      "New Output data: \n",
      "[0.9956062]\n",
      "Wow, we did it!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #initializing the neuron class\n",
    "    neural_network = NeuralNetwork()\n",
    "\n",
    "    print(\"Beginning Randomly Generated Weights: \")\n",
    "    print(neural_network.synaptic_weights)\n",
    "\n",
    "    #training data consisting of 4 examples--3 input values and 1 output\n",
    "    training_inputs = np.array([[0,0,1],\n",
    "                                [1,1,1],\n",
    "                                [1,0,1],\n",
    "                                [0,1,1]])\n",
    "\n",
    "    training_outputs = np.array([[0,1,1,0]]).T\n",
    "\n",
    "    #training taking place\n",
    "    neural_network.train(training_inputs, training_outputs, 15000)\n",
    "\n",
    "    print(\"Ending Weights After Training: \")\n",
    "    print(neural_network.synaptic_weights)\n",
    "\n",
    "    user_input_one = str(input(\"User Input One: \"))\n",
    "    user_input_two = str(input(\"User Input Two: \"))\n",
    "    user_input_three = str(input(\"User Input Three: \"))\n",
    "    \n",
    "    print(\"Considering New Situation: \", user_input_one, user_input_two, user_input_three)\n",
    "    print(\"New Output data: \")\n",
    "    print(neural_network.think(np.array([user_input_one, user_input_two, user_input_three])))\n",
    "    print(\"Wow, we did it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "1. How many layers were there in the above neural network? \n",
    "2. How many neurons were there in each layer?\n",
    "3. Is this an effective neural network to use in a real life example?\n",
    "4. How can we improve this neural network to get better performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to modify the code by changing the error function, input values, adding more neurons, adding more layers, or anything else, and see if you can improve the prediction of the neural network!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
