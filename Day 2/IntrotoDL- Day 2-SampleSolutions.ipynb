{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL workshop (Answer Key).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUyCu2-hTkX1",
        "colab_type": "text"
      },
      "source": [
        "Everything below is only a **sample** solution and is **not** the only model or the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4KUrPQdI8OP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import imdb\n",
        "from tensorflow.keras.layers import (Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization)\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPZHBSlMbUiy",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4CoZ2k7bgMF",
        "colab_type": "text"
      },
      "source": [
        "You want to buy a ring for yourself. You enter a store and they tell you the costing method of the ring. It costs 500 dollars to make the ring and it costs an extra 100 dollars for adding one diamond to the ring. So it would cost 600 dollars for a ring with 1 diamond and 1100 dollars for a ring with 6 diamonds. Create a model which takes an input for number of diamonds and outputs the cost\n",
        "\n",
        "Hint:<br/>\n",
        "1) use a small scale value(have your output in terms of thousands of dollars)<br/>\n",
        "2) Bigger the dataset, better the prediction<br/>\n",
        "3) In this case, number of epochs will improve your prediction but it leads to overfitting (we will discuss this later in the workshop) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_jU1AJtbjHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xs=[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "ys=[0.5,0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3,1.4]\n",
        "model = Sequential([Dense(units=1, input_shape=[1])])\n",
        "model.compile(optimizer = 'sgd', loss = 'mean_squared_error')\n",
        "model.fit(xs,ys,epochs = 400)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e3OkMeTcS4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.predict([0]))\n",
        "print(model.predict([5]))\n",
        "print(model.predict([10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qydGctdycVf7",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P44oVOMkciVi",
        "colab_type": "text"
      },
      "source": [
        "Create a convolutional neural network to classify images of the Fashion MNIST dataset.\n",
        "\n",
        "Fashion-MNIST is a dataset of Zalando's article images consisting of a training set of 60,000 examples and a test set of 10,000 examples.\n",
        "\n",
        "Each example is a 28x28 grayscale image, associated with a label from 10 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qpx0mNY-VHlE"
      },
      "source": [
        "**Load dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0dn3E2WeLpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load data from fashion_mnist into sets for training and testing\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "#assign names to labels\n",
        "class_names={0:'top', 1:'trouser', 2:'pullover', 3:'dress', 4:'coat', 5:'sandal', 6:'shirt', 7:'sneaker', 8:'bag', 9:'boot'}\n",
        "\n",
        "#visualize data\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(9):\n",
        "    plt.subplot(3,3,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap='binary')\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5uUiyqWdWWQ",
        "colab_type": "text"
      },
      "source": [
        "**Pre-process data** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjzS9q3SI6vF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#normalize the images\n",
        "train_norm = train_images/ 255.0\n",
        "test_norm = test_images/ 255.0\n",
        "\n",
        "#reshape the images\n",
        "train_norm = train_norm.reshape((train_norm.shape[0], 28, 28, 1))\n",
        "test_norm = test_norm.reshape((test_norm.shape[0], 28, 28, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUik3_qFdl5M",
        "colab_type": "text"
      },
      "source": [
        "**Build model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyJdgGvJI822",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=3, activation='relu'),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTxkbNoadozr",
        "colab_type": "text"
      },
      "source": [
        "**Train model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qZX-ofwbI9Uj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer = 'adam', loss= 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.fit(train_norm, train_labels, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15ZICl5qdsDw",
        "colab_type": "text"
      },
      "source": [
        "**Evaluate perfomance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koCOUiQ0I91Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss, acc = model.evaluate(train_norm,  train_labels)\n",
        "print(\"Training : loss={:.3f} - acc={:.3f}\".format(loss, acc))\n",
        "\n",
        "loss, acc = model.evaluate(test_norm,  test_labels)\n",
        "print(\"Test : loss={:.3f} - acc={:.3f}\".format(loss, acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H_e_xfOyPjD4",
        "colab": {}
      },
      "source": [
        "predictions = np.argmax(model.predict(test_norm), axis=1) \n",
        "\n",
        "#visualize predictions \n",
        "plt.figure(figsize=(10, 10))\n",
        "for i in range(50):\n",
        "    plt.subplot(5,10,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(test_images[i], cmap='binary')\n",
        "    actual_class=class_names[test_labels[i]]\n",
        "    if predictions[i]==test_labels[i]:\n",
        "      plt.xlabel(actual_class, color='green')\n",
        "    else:\n",
        "      plt.xlabel(actual_class, color='red')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRR1-nQaeAmj",
        "colab_type": "text"
      },
      "source": [
        "**Improve model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KobHl0xcPsb3",
        "colab_type": "text"
      },
      "source": [
        "Added a convolutional layer and dropout layers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2D71xlTI-fz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_improved = Sequential([\n",
        "    Conv2D(32, kernel_size=3, activation='relu'),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Dropout(0.25),\n",
        "    Conv2D(64, kernel_size=3, activation='relu'),\n",
        "    Dropout(0.25),\n",
        "    Flatten(),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aF-TH60RLz5n",
        "colab": {}
      },
      "source": [
        "model_improved.compile(optimizer = 'adam', loss= 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
        "model_improved.fit(train_norm, train_labels, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNE0VHJHecKx",
        "colab_type": "text"
      },
      "source": [
        "**Re-evaluate performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4R-753vcLnBT",
        "colab": {}
      },
      "source": [
        "loss, acc = model_improved.evaluate(train_norm,  train_labels)\n",
        "print(\"Training : loss={:.3f} - acc={:.3f}\".format(loss, acc))\n",
        "\n",
        "loss, acc = model_improved.evaluate(test_norm,  test_labels)\n",
        "print(\"Test : loss={:.3f} - acc={:.3f}\".format(loss, acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H-Atpb0NM8Cq",
        "colab": {}
      },
      "source": [
        "predictions = np.argmax(model_improved.predict(test_norm), axis=1) \n",
        "\n",
        "#visualize predictions \n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(50):\n",
        "    plt.subplot(5,10,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(test_images[i], cmap='binary')\n",
        "    actual_class=class_names[test_labels[i]]\n",
        "    if predictions[i]==test_labels[i]:\n",
        "      plt.xlabel(actual_class, color='green')\n",
        "    else:\n",
        "      plt.xlabel(actual_class, color='red')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nKab4kiHo8gd"
      },
      "source": [
        "### Exercise 3 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4oINNwHp0XP",
        "colab_type": "text"
      },
      "source": [
        "Create a model to categorise movie reviews of the [IMDB dataset](https://ai.stanford.edu/~amaas/data/sentiment/).\n",
        "\n",
        "This is a dataset of movies reviews from IMDB, labeled by sentiment (1 for positive and 0 for negative) consisting of 25000 training examples and 25000 testing examples.\n",
        "\n",
        "Reviews have been preprocessed, and each review is encoded as a list of word indices (integers). Words are indexed by overall frequency in the dataset, for instance the integer \"3\" encodes the 3rd most frequently occuring word.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m1Nyiutx-vq",
        "colab_type": "text"
      },
      "source": [
        "**Load dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwXaRZpm5SDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#only the num_words most frequent words are kept\n",
        "#any less frequent word will appear as out-of-vocabulary character value, by default 2, in the sequence \n",
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
        "\n",
        "#merge training examples and testing examples, so that we can change from 50/50 to 80/20 split distribution\n",
        "data = np.concatenate((training_data, testing_data), axis=0)\n",
        "targets = np.concatenate((training_targets, testing_targets), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1hAmuG6x8hi",
        "colab_type": "text"
      },
      "source": [
        "**Pre-process data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5npPa5SsMoy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#to visualize predictions later, keep a copy of non-vectorized test examples\n",
        "check_x = data[:10000]\n",
        "check_y = targets[:10000]\n",
        "\n",
        "def vectorize(sequences, dimension = 10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1\n",
        "  return results\n",
        "\n",
        "#vectorize reviews and labels\n",
        "data = vectorize(data)\n",
        "targets = np.array(targets).astype(\"float32\")\n",
        "#10K reviews for testing\n",
        "test_x = data[:10000]\n",
        "test_y = targets[:10000]\n",
        "#40K reviews for training\n",
        "train_x = data[10000:]\n",
        "train_y = targets[10000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrz6DuJZv25a",
        "colab_type": "text"
      },
      "source": [
        "**Build model**\n",
        "\n",
        "Sample model below is not a CNN, but you can make one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi_Dk4dSwDMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential([\n",
        "      Dense(50, activation = \"relu\", input_shape=(10000, )),\n",
        "      Dropout(0.25),\n",
        "      Dense(50, activation = \"relu\"),\n",
        "      Dropout(0.25),\n",
        "      Dense(50, activation = \"relu\"),\n",
        "      Dense(1, activation = \"sigmoid\")\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkPQmpzkv3K9",
        "colab_type": "text"
      },
      "source": [
        "**Train model and evaluate performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VfuuGfDtGIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
        "\n",
        "results = model.fit(train_x, train_y, epochs= 2, batch_size = 500, validation_data = (test_x, test_y))\n",
        " #batch size is number of training examples utilized in one iteration, improves performance in many scenarios\n",
        "print(\"Test-Accuracy:\", np.mean(results.history[\"accuracy\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg3SbvTW5TMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the get_word_index function retrieves a dictionary mapping words to their index in the dataset\n",
        "index = imdb.get_word_index()     \n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()]) \n",
        "\n",
        "#to read the review\n",
        "#note that indices were offset by 3, because 0, 1 and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\"\n",
        "decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in check_x[0]] )\n",
        "print(decoded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o02XVVg__Ujj",
        "colab_type": "text"
      },
      "source": [
        "**Predict**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUuBpwFZapIs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#binary classification\n",
        "sentiment = {1: 'positive', 0:'negative'}\n",
        "\n",
        "#check prediction for the review you have read above\n",
        "predictions = model.predict(test_x)\n",
        "\n",
        "#output of your network should be a scalar between 0 and 1, encoding a probability \n",
        "#output is the likelihood of reviews being positive\n",
        "predictions = np.round(predictions).astype('int64')\n",
        "print(\"predicted: \", sentiment[predictions[0][0]])\n",
        "print(\"actual: \", sentiment[check_y[0]])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}